{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Response Modeling of Bank Marketing Campaign_\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src=\"AI.png\" width = '400'><br>\n",
    "\n",
    "\n",
    "### _Business Scenario_\n",
    "\n",
    "There has been a revenue decline for the Portuguese bank and they would like to know what actions to take. After investigation, we found out that the root cause is that their clients are not depositing as frequently as before. Knowing that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. In addition, banks also hold better chance to persuade term deposit clients into buying other products such as funds or insurance to further increase their revenues. As a result, the Portuguese bank would like to identify existing clients that have higher chance to subscribe for a term deposit and focus marketing effort on such clients.\n",
    "\n",
    "\n",
    "* The task is to build a POC for the problem\n",
    "\n",
    "* The data is related with direct marketing campaigns of a Portuguese banking institution. \n",
    "\n",
    "* The marketing campaigns were based on phone calls. \n",
    "\n",
    "* Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Attributes Information_\n",
    "\n",
    "\n",
    "### _Bank client data:_\n",
    "1 - age (numeric)\n",
    "\n",
    "2 - job : type of job \n",
    "(categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "\n",
    "3 - marital : marital status \n",
    "(categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "\n",
    "4 - education (categorical:'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "### _Data Related to the last contact of the current campaign:_\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone') \n",
    "\n",
    "9 - month: last contact month of year \n",
    "(categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "\n",
    "10 - day_of_week: last contact day of the week \n",
    "(categorical: 'mon','tue','wed','thu','fri')\n",
    "\n",
    "11 - duration: last contact duration, in seconds (numeric). \n",
    "Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "### _Other attributes:_\n",
    "\n",
    "12 - campaign: number of contacts performed during this campaign and for this client \n",
    "(numeric, includes last contact)\n",
    "\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign \n",
    "(numeric; 999 means client was not previously contacted)\n",
    "\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "### _Social and economic context attributes_\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric) \n",
    "\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n",
    "\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Directory\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Exploratory Analysis_\n",
    "\n",
    "### _Import Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install imblearn\n",
    "\n",
    "#if the above command does not work to install imblearn package run the following command in your terminal\n",
    "# conda install -c glemaitre imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.13.3\nyour numpy version is 1.11.3.\nPlease upgrade numpy to >= 1.13.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-68118e3876e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# numpy compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from pandas.compat.numpy import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0m_np_version_under1p14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0m_np_version_under1p15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m\"your numpy version is {0}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m\"Please upgrade numpy to >= 1.13.3 to use \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;34m\"this pandas version\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_np_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.13.3\nyour numpy version is 1.11.3.\nPlease upgrade numpy to >= 1.13.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_for_sklearn(label_list):\n",
    "    return [1 if i == 'yes' else 0 for i in label_list]\n",
    "\n",
    "def accuracy_precision_recall_metrics(y_true, y_pred):\n",
    "    \n",
    "    y_test_scoring = convert_for_sklearn(y_test)\n",
    "    test_pred_scoring = convert_for_sklearn(y_pred)\n",
    "\n",
    "    acc = accuracy_score(y_true= y_test_scoring, y_pred = test_pred_scoring)\n",
    "    prec = precision_score(y_true= y_test_scoring, y_pred = test_pred_scoring)\n",
    "    rec = recall_score(y_true= y_test_scoring, y_pred = test_pred_scoring)\n",
    "    \n",
    "    print(\"Test Precision: \",acc)\n",
    "    print(\"Test Recall: \",prec)\n",
    "    print(\"Test Accuracy: \",rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Read in the data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv(\"bank-additional-full.csv\", sep=',', header=0, na_values='unknown')\n",
    "test_data =  pd.read_csv(\"test_cases.csv\", sep=',', header=0, na_values='unknown')\n",
    "\n",
    "print(bank_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Understand the dataset_\n",
    "\n",
    "Undestanding the dataset can be as thorough as you want it to be, you can start by looking at the variables and asking questions, like the one's mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the names of the columns?\n",
    "print(list(bank_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data types?\n",
    "bank_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the distribution of numerical columns?\n",
    "bank_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about categorical variable levels count?\n",
    "bank_data.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.marital.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Distribition of dependent variable_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Distribution\n",
    "sns.countplot(x='y',data=bank_data)\n",
    "plt.show()\n",
    "\n",
    "# What are the counts?\n",
    "print(bank_data.y.value_counts())\n",
    "\n",
    "# What is the percentage?\n",
    "count_yes = len(bank_data[bank_data.y == 'yes'])\n",
    "count_no = len(bank_data[bank_data.y != 'yes'])\n",
    "\n",
    "percent_success = (count_yes/(count_yes + count_no))*100\n",
    "\n",
    "print('Percentage of people who have taken the campaign:', percent_success, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Distribition of Other Variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"credit_default\", data=bank_data)\n",
    "plt.show()\n",
    "\n",
    "bank_data.credit_default.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when someone is subscribing for the campaign, is it on a particular day of the week?\n",
    "bank_data[bank_data.y == 'yes'].day_of_week.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data[bank_data.y == 'yes'].marital.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Drawing trends toward the target variable_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.groupby('y').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of age\n",
    "%matplotlib inline\n",
    "sns.distplot(bank_data[\"age\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the kind of job matter to a subscription? (groupby job and aggregate by mean) \n",
    "# Extend to education, marital, etc.\n",
    "\n",
    "bank_data.groupby('job').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who are subscribing the most across job ranks?\n",
    "bank_data[bank_data.y == 'yes'].job.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Some Insights from the EDA done above_\n",
    "\n",
    "* Number of Campaigns are lower for people who subscribed!\n",
    "\n",
    "* People who subscribed for a term deposit are older (average age)\n",
    "\n",
    "* customer_no attribute looks to be providing no value. Hence can be removed\n",
    "\n",
    "* Few attributes such as job, marital, education, credit_default, housing, loan, contact, contacted_month, day_of_week, poutcome and y are categorical but are interpreted as object type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Feature Engineering_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Fix levels of categorical variable by domain_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check levels of education. Is there anything wrong?\n",
    "bank_data.education.value_counts()\n",
    "\n",
    "# clean up basic level \n",
    "bank_data.replace(['basic.6y','basic.4y', 'basic.9y'], 'basic', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.education.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.education.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Drop Unnecessary variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.drop(\"customer_no\", axis = 1, inplace= True)\n",
    "test_data.drop(\"customer_no\", axis = 1, inplace= True)\n",
    "\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Type Casting_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['job', 'marital', 'education', 'credit_default', 'housing', 'loan', 'contact', 'contacted_month', 'day_of_week', 'poutcome', 'y']:\n",
    "    bank_data[col] = bank_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How to choose between category and object:\n",
    "\n",
    "reference : https://stackoverflow.com/questions/30601830/when-to-use-category-rather-than-object\n",
    "\n",
    "The categorical data type is useful in the following cases:\n",
    "\n",
    "* A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory\n",
    "* The lexical order of a variable is not the same as the logical order (“one”, “two”, “three”). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order\n",
    "* As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Numeric and Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attr = list(bank_data.select_dtypes(\"category\").columns)\n",
    "num_attr = list(bank_data.columns.difference(cat_attr))\n",
    "\n",
    "cat_attr.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Handle Missing Values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print columns with missing values\n",
    "missing_cols = bank_data.columns[bank_data.isnull().any()]\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn pipelines\n",
    "\n",
    "Pipelines are a way to streamline a lot of the routine processes, encapsulating little pieces of logic into one function call, which makes it easier to actually do modeling instead just writing a bunch of code.\n",
    "\n",
    "Pipelines are set up with the fit/transform/predict functionality, so you can fit a whole pipeline to the training data and transform to the test data, without having to do it individually for each thing you do. Super convenienent, right?\n",
    "\n",
    "Steps to follow to create a pipeline\n",
    "\n",
    "Step 1) Fetch the numerical and categorical columns\n",
    "\n",
    "Step 2) Create a transformer/pipeline for numerical attributes\n",
    "\n",
    "    Create a list of tuples where each tuple represents the operation to be performed on numerical attributes\n",
    "\n",
    "Step 3) Create a transformer/pipeline for categorical attributes\n",
    "\n",
    "    Create a list of tuples where each tuple represent the operation to be performed on categorical attributes\n",
    "\n",
    "Step 4) Create a ColumnTransformer which merges both the numerical and categorical transformers\n",
    "\n",
    "Step5) Create a final pipeline object which includes the ColumnTransformer and an estimator (an algorithm to be build on dataset)\n",
    "\n",
    "Step6) (optional) Create a GridSearchCV object with pipeline as one of the inputs along with hyperparameter grid and Cross validation object\n",
    "\n",
    "Step7) Apply fit() on train data and predict() on test data <br><br>\n",
    "\n",
    "**TL; DR**\n",
    "Pipeline is a collection of transformers chained together and operate sequentially. (often ending with an estimator)\n",
    "\n",
    "__Bird's view of sklearn pipeline__\n",
    "\n",
    "<img src=\"Pipeline_broadview.png\"><br><br>\n",
    "\n",
    "__Train and Test dataflow inside the sklearn pipeline__\n",
    "<img src=\"fit_tranform.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Instantiate Pre-processing Objects for Pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_attr),\n",
    "        ('cat', categorical_transformer, cat_attr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Instantiate Pipeline Object_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logreg = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Train-Test Split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = bank_data.loc[:,bank_data.columns!='y'], bank_data.loc[:,'y']\n",
    "\n",
    "X_test, y_test = test_data.loc[:,test_data.columns!='y'], test_data.loc[:,'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Build Logistic Regression Model - 1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Evaluate Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = clf_logreg.predict(X_train)\n",
    "test_pred = clf_logreg.predict(X_test)\n",
    "\n",
    "print(clf_logreg.score(X_train, y_train))\n",
    "print(clf_logreg.score(X_test, y_test))\n",
    "\n",
    "print(confusion_matrix(y_true=y_train, y_pred = train_pred, labels = ['no', 'yes']))\n",
    "\n",
    "confusion_matrix_test = confusion_matrix(y_true=y_test, y_pred =  test_pred)\n",
    "confusion_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_test=(confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "\n",
    "Precision_Test= confusion_matrix_test[1,1]/(confusion_matrix_test[1,1]+confusion_matrix_test[0,1])\n",
    "Recall_Test= confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "\n",
    "print(\"Test Precision: \",Precision_Test)\n",
    "print(\"Test Recall: \",Recall_Test)\n",
    "print(\"Test Accuracy: \",Accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the function to calculate accuracy, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_precision_recall_metrics(y_true = y_test, y_pred = test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Build Decision Tree Model - 2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_dt = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "dt_param_grid = {'classifier__criterion': ['entropy', 'gini'], 'classifier__max_depth': [6,8,10,12], \n",
    "                 \"classifier__min_samples_split\": [2, 10, 20],\"classifier__min_samples_leaf\": [1, 5, 10]}\n",
    "\n",
    "dt_grid = GridSearchCV(clf_dt, param_grid=dt_param_grid, cv=5)\n",
    "\n",
    "dt_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = dt_grid.predict(X_train)\n",
    "test_pred = dt_grid.predict(X_test)\n",
    "\n",
    "print(dt_grid.score(X_train, y_train))\n",
    "print(dt_grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using the function to calculate accuracy, precision and recall.\n",
    "\n",
    "accuracy_precision_recall_metrics(y_true = y_test, y_pred = test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Build Random Forest Model - 3_ (Using Stratified KFold)\n",
    "\n",
    "__Stratified K-Folds cross-validator__\n",
    "\n",
    "This cross-validation object is a **variation** of KFold that returns stratified folds. The folds are made by **preserving the percentage of samples for each class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=143)\n",
    "\n",
    "param_grid = {\"classifier__n_estimators\" : [150, 250, 300],\n",
    "              \"classifier__max_depth\" : [5,8,10],\n",
    "              \"classifier__max_features\" : [3, 5, 7],\n",
    "              \"classifier__min_samples_leaf\" : [4, 6, 8, 10]}\n",
    "\n",
    "rf_grid = GridSearchCV(clf, param_grid=dt_param_grid, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = rf_grid.predict(X_train)\n",
    "test_pred = rf_grid.predict(X_test)\n",
    "\n",
    "print(rf_grid.score(X_train, y_train))\n",
    "print(rf_grid.score(X_test, y_test))\n",
    "\n",
    "#### Using the function to calculate accuracy, precision and recall.\n",
    "\n",
    "accuracy_precision_recall_metrics(y_true = y_test, y_pred = test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Build Gradient Boosting - 4_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('GBM',GradientBoostingClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm_param_grid = {'GBM__max_depth': [8,10,12,14], 'GBM__subsample': [0.8, 0.6,], 'GBM__max_features':[0.2, 0.3], \n",
    "              'GBM__n_estimators': [10, 20, 30]}\n",
    "\n",
    "gbm_grid = GridSearchCV(clf, param_grid=gbm_param_grid, cv=3)\n",
    "\n",
    "gbm_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = gbm_grid.predict(X_train)\n",
    "test_pred = gbm_grid.predict(X_test)\n",
    "\n",
    "print(gbm_grid.score(X_train, y_train))\n",
    "print(gbm_grid.score(X_test, y_test))\n",
    "\n",
    "#### Using the function to calculate accuracy, precision and recall.\n",
    "\n",
    "accuracy_precision_recall_metrics(y_true = y_test, y_pred = test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Handling Imbalanced Data_\n",
    "\n",
    "### _1. Class Weights of loss function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_dt = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "dt_param_grid = {'classifier__criterion': ['entropy', 'gini'], 'classifier__max_depth': [6,8,10,12], \n",
    "                 \"classifier__min_samples_split\": [2, 10, 20],\"classifier__min_samples_leaf\": [1, 5, 10],\n",
    "                 \"classifier__class_weight\":['balanced']}\n",
    "\n",
    "dt_grid_bal = GridSearchCV(clf_dt, param_grid=dt_param_grid, cv=5)\n",
    "dt_grid_bal.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = dt_grid_bal.predict(X_train)\n",
    "test_pred = dt_grid_bal.predict(X_test)\n",
    "\n",
    "print(dt_grid_bal.score(X_train, y_train))\n",
    "print(dt_grid_bal.score(X_test, y_test))\n",
    "\n",
    "#### Using the function to calculate accuracy, precision and recall.\n",
    "# accuracy_precision_recall_metrics(y_true = y_train, y_pred= train_pred)\n",
    "accuracy_precision_recall_metrics(y_true = y_test, y_pred = test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2. Oversample Using SMOTE_\n",
    "\n",
    "<img src=\"SMOTE.jpg\" width = '400'><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "X_train_pp = pd.DataFrame(clf.fit_transform(X_train))\n",
    "X_test_pp = pd.DataFrame(clf.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "\n",
    "\n",
    "os_data_X,os_data_y=smote.fit_sample(X_train_pp, y_train)\n",
    "\n",
    "os_data_X = pd.DataFrame(data=os_data_X)\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])\n",
    "\n",
    "\n",
    "os_data_X = pd.DataFrame(data=os_data_X)\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])\n",
    "\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print(\"Number of no subscription in oversampled data\",len(os_data_y[os_data_y['y']=='no']))\n",
    "print(\"Number of subscription\",len(os_data_y[os_data_y['y']=='yes']))\n",
    "print(\"Proportion of no subscription data in oversampled data is \",len(os_data_y[os_data_y['y']=='no'])/len(os_data_X))\n",
    "print(\"Proportion of subscription data in oversampled data is \",len(os_data_y[os_data_y['y']=='yes'])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_dt = Pipeline(steps=[('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "dt_param_grid = {'classifier__criterion': ['entropy', 'gini'], 'classifier__max_depth': [6,8,10,12], \n",
    "                 \"classifier__min_samples_split\": [2, 10, 20],\"classifier__min_samples_leaf\": [1, 5, 10]}\n",
    "\n",
    "dt_grid_bal = GridSearchCV(clf_dt, param_grid=dt_param_grid, cv=5)\n",
    "\n",
    "dt_grid_bal.fit(os_data_X,os_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = dt_grid_bal.predict(os_data_X).reshape(1,-1)[0]\n",
    "test_pred = dt_grid_bal.predict(X_test_pp).reshape(1,-1)[0]\n",
    "\n",
    "print(dt_grid_bal.score(os_data_X, os_data_y))\n",
    "print(dt_grid_bal.score(X_test_pp, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_train = confusion_matrix(y_true=os_data_y, y_pred = train_pred)\n",
    "confusion_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_train=(confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(np.sum(confusion_matrix_train))\n",
    "\n",
    "Precision_Train= confusion_matrix_train[1,1]/(confusion_matrix_train[1,1]+confusion_matrix_train[0,1])\n",
    "Recall_Train= confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "\n",
    "print(\"Test Precision: \",Precision_Train)\n",
    "print(\"Test Recall: \",Recall_Train)\n",
    "print(\"Train Accuracy: \",Accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_precision_recall_metrics(y_true = y_test, y_pred = test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Insights_\n",
    "\n",
    "* Excel Sheet Analysis\n",
    "\n",
    "\n",
    "* Positive : Yes to Campaign ; Negative : No to Campaign\n",
    "\n",
    "\n",
    "* Recall Importance : False pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm.\n",
    "#### The XGBoost Advantages\n",
    "\n",
    "-  Parallel Processing:\n",
    "    -  XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
    "    -  Boosting is sequential process that each tree can be built only after the previous one, and it  can be parallelized with making a tree using all cores. Refer. http://zhanpengfang.github.io/418home.html\n",
    "    \n",
    "    -  XGBoost also supports implementation on Hadoop.\n",
    "-  High Flexibility\n",
    "    -  XGBoost allow users to define custom optimization objectives and evaluation criteria.\n",
    "    -  This adds a whole new dimension to the model and there is no limit to what we can do.\n",
    "-  Handling Missing Values\n",
    "    -  XGBoost has an in-built routine to handle missing values.\n",
    "    -  User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n",
    "-  Tree Pruning:\n",
    "    -  A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.\n",
    "    -  XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "    -  Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
    "-  Built-in Cross-Validation\n",
    "    -  XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "    -  This is unlike GBM where we have to run a grid-search and only a limited values can be tested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
